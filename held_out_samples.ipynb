{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import InterpolationMode\n",
    "mean, std = (0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.262)\n",
    "transform_train_proper = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std),\n",
    "        ])\n",
    "transform_blurred = transforms.Compose([\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Resize(8, interpolation=InterpolationMode.BILINEAR, antialias=None), \n",
    "    transforms.Resize(32, interpolation=InterpolationMode.BILINEAR, antialias=None),\n",
    "    transforms.Normalize(mean, std)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.datasets import get_cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import utils\n",
    "DATASET_NAME = 'cifar10'\n",
    "\n",
    "def get_held_out_data(dataset_name, nb_samples=50):\n",
    "    train_dataset, _, _ = get_cifar10()\n",
    "    x_data = np.array(train_dataset.data)\n",
    "    y_data = np.array(train_dataset.targets)\n",
    "    num_classes = len(np.unique(y_data))\n",
    "    nb_samples_per_class = nb_samples // num_classes\n",
    "    idxs = []\n",
    "    for i in range(num_classes):\n",
    "        idxs_i = np.where(y_data == i)[0]\n",
    "        sampled_idxs_i = np.random.choice(idxs_i, size=nb_samples_per_class, replace=False)\n",
    "        idxs.append(sampled_idxs_i)\n",
    "        \n",
    "    idxs = np.concatenate(idxs)\n",
    "    x_data = x_data[idxs]\n",
    "    y_data = y_data[idxs]\n",
    "    \n",
    "    if not os.path.exists('data'):\n",
    "        os.mkdir('data')\n",
    "    np.save(f'data/{dataset_name}_held_out_x.npy', x_data)\n",
    "    np.save(f'data/{dataset_name}_held_out_y.npy', y_data)\n",
    "    return x_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "x_data, y_data = get_held_out_data(DATASET_NAME, nb_samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data_proper = [transform_train_proper(x) for x in x_data]\n",
    "x_data_proper = torch.stack(x_data_proper)\n",
    "y_data_proper = torch.from_numpy(y_data)\n",
    "\n",
    "torch.save(x_data_proper, f'data/{DATASET_NAME}_held_out_proper_x.pt')\n",
    "torch.save(y_data_proper, f'data/{DATASET_NAME}_held_out_proper_y.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data_blurred = [transform_blurred(x) for x in x_data]\n",
    "x_data_blurred = torch.stack(x_data_blurred)\n",
    "y_data_blurred = torch.from_numpy(y_data)\n",
    "\n",
    "torch.save(x_data_blurred, f'data/{DATASET_NAME}_held_out_blurred_x.pt')\n",
    "torch.save(y_data_blurred, f'data/{DATASET_NAME}_held_out_blurred_y.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.prepare import prepare_loaders\n",
    "batch_size = 500\n",
    "loaders_params = {'batch_size': batch_size, 'pin_memory': True, 'num_workers': 8}\n",
    "loaders = prepare_loaders(DATASET_NAME, {}, loaders_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(loaders['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(y.cpu().numpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = x_data_proper.reshape(x_data_proper.size(0), -1)\n",
    "x_data /= x_data.norm(dim=1, keepdim=True)\n",
    "sim_m = x_data @ x_data.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_info(cluster_labels, y_train):\n",
    "    # Initializing\n",
    "    unsolicited_ratio = 0.0\n",
    "    denominator = 0.0\n",
    "    reference_labels = {}\n",
    "    # For loop to run through each label of cluster label\n",
    "    for label in range(len(np.unique(y_train))):\n",
    "        index = np.where(cluster_labels==label, 1, 0)\n",
    "        dist = np.bincount(y_train[index==1])\n",
    "        num = dist.argmax()\n",
    "        unsolicited_ratio += (dist.sum() - dist.max())\n",
    "        denominator += dist.sum()\n",
    "        reference_labels[label] = num\n",
    "    proper_labels = [reference_labels[label] for label in cluster_labels]\n",
    "    proper_labels = np.array(proper_labels)\n",
    "    unsolicited_ratio /= denominator\n",
    "    return proper_labels, unsolicited_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering \n",
    "\n",
    "similarity_matrix_ = sim_m.cpu().numpy()\n",
    "labels_pred = SpectralClustering(n_clusters=10, affinity='precomputed', n_init=100, assign_labels='discretize').fit_predict((1+similarity_matrix_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_pred, unsolicited_ratio = retrieve_info(labels_pred, y_data)\n",
    "unsolicited_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data_proper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = (labels_pred == y_data_proper.numpy()).astype(float).sum() / y_data_proper.shape[0]\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(labels_pred == y_data_proper.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([4, 5, 6]),\n",
       "indices=tensor([1, 1, 1]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[1, 2, 3], [4, 5, 6]]).max(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root=os.environ['CIFAR10_PATH'], train=True,\n",
    "                                        download=False, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root=os.environ['CIFAR10_PATH'], train=False,\n",
    "                                       download=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(iter(testloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.025"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2.5e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0., 160., 320., 480., 640., 800.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.linspace(0, 800, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.datasets import get_cifar10\n",
    "DATASET_NAME = 'cifar10'\n",
    "\n",
    "def get_held_out_data(dataset_name, nb_samples=50):\n",
    "    train_dataset, _, _ = get_cifar10()\n",
    "    y_data = np.array(train_dataset.targets)\n",
    "    print(len(y_data))\n",
    "    num_classes = len(np.unique(y_data))\n",
    "    nb_samples_per_class = nb_samples // num_classes\n",
    "    idxs = []\n",
    "    for i in range(num_classes):\n",
    "        idxs_i = np.where(y_data == i)[0]\n",
    "        sampled_idxs_i = np.random.choice(idxs_i, size=nb_samples_per_class, replace=False)\n",
    "        idxs.append(sampled_idxs_i)\n",
    "        \n",
    "    idxs = np.concatenate(idxs)\n",
    "    \n",
    "    if not os.path.exists('data'):\n",
    "        os.mkdir('data')\n",
    "    np.save(f'data/{dataset_name}_idxs.npy', idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "get_held_out_data(DATASET_NAME, nb_samples=50000//2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_path = 'data/cifar10_idxs.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "ename": "UnpicklingError",
     "evalue": "STACK_GLOBAL requires str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m/raid/NFS_SHARE/home/b.krzepkowski/Github/NeuralCollapse/held_out_samples.ipynb Cell 32\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdgx0/raid/NFS_SHARE/home/b.krzepkowski/Github/NeuralCollapse/held_out_samples.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m get_cifar10\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdgx0/raid/NFS_SHARE/home/b.krzepkowski/Github/NeuralCollapse/held_out_samples.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m dataset_params \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mdataset_path\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mNone\u001b[39;00m, \u001b[39m'\u001b[39m\u001b[39mwhether_aug\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mTrue\u001b[39;00m, \u001b[39m'\u001b[39m\u001b[39mproper_normalization\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mTrue\u001b[39;00m, \u001b[39m'\u001b[39m\u001b[39msubset_path\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mdata/cifar10_idxs.npy\u001b[39m\u001b[39m'\u001b[39m}\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bdgx0/raid/NFS_SHARE/home/b.krzepkowski/Github/NeuralCollapse/held_out_samples.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m datasets \u001b[39m=\u001b[39m get_cifar10(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mdataset_params)\n",
      "File \u001b[0;32m~/Github/NeuralCollapse/src/data/datasets.py:45\u001b[0m, in \u001b[0;36mget_cifar10\u001b[0;34m(dataset_path, whether_aug, proper_normalization, subset_path)\u001b[0m\n\u001b[1;32m     38\u001b[0m transform_train \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39mCompose([\n\u001b[1;32m     39\u001b[0m     transforms\u001b[39m.\u001b[39mTrivialAugmentWide(),\n\u001b[1;32m     40\u001b[0m     transforms\u001b[39m.\u001b[39mToTensor(),\n\u001b[1;32m     41\u001b[0m     transforms\u001b[39m.\u001b[39mNormalize(mean, std),\n\u001b[1;32m     42\u001b[0m     transforms\u001b[39m.\u001b[39mRandomErasing(p\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m),\n\u001b[1;32m     43\u001b[0m ])\n\u001b[1;32m     44\u001b[0m train_dataset \u001b[39m=\u001b[39m datasets\u001b[39m.\u001b[39mCIFAR10(dataset_path, train\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, download\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, transform\u001b[39m=\u001b[39mTRANSFORMS_NAME_MAP[\u001b[39m'\u001b[39m\u001b[39mtransform_proper_train\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 45\u001b[0m \u001b[39mif\u001b[39;00m subset_path \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m     selected_indices \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(np\u001b[39m.\u001b[39mload(subset_path))\n\u001b[1;32m     47\u001b[0m     train_dataset \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mSubset(train_dataset, selected_indices)\n",
      "File \u001b[0;32m~/miniconda3/envs/ncollapse/lib/python3.10/site-packages/torch/serialization.py:815\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    814\u001b[0m         \u001b[39mraise\u001b[39;00m pickle\u001b[39m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e)) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 815\u001b[0m \u001b[39mreturn\u001b[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpickle_load_args)\n",
      "File \u001b[0;32m~/miniconda3/envs/ncollapse/lib/python3.10/site-packages/torch/serialization.py:1033\u001b[0m, in \u001b[0;36m_legacy_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1027\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(f, \u001b[39m'\u001b[39m\u001b[39mreadinto\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mand\u001b[39;00m (\u001b[39m3\u001b[39m, \u001b[39m8\u001b[39m, \u001b[39m0\u001b[39m) \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m sys\u001b[39m.\u001b[39mversion_info \u001b[39m<\u001b[39m (\u001b[39m3\u001b[39m, \u001b[39m8\u001b[39m, \u001b[39m2\u001b[39m):\n\u001b[1;32m   1028\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1029\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtorch.load does not work with file-like objects that do not implement readinto on Python 3.8.0 and 3.8.1. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1030\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mReceived object of type \u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(f)\u001b[39m}\u001b[39;00m\u001b[39m\\\"\u001b[39;00m\u001b[39m. Please update to Python 3.8.2 or newer to restore this \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1031\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mfunctionality.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1033\u001b[0m magic_number \u001b[39m=\u001b[39m pickle_module\u001b[39m.\u001b[39;49mload(f, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpickle_load_args)\n\u001b[1;32m   1034\u001b[0m \u001b[39mif\u001b[39;00m magic_number \u001b[39m!=\u001b[39m MAGIC_NUMBER:\n\u001b[1;32m   1035\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mInvalid magic number; corrupt file?\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mUnpicklingError\u001b[0m: STACK_GLOBAL requires str"
     ]
    }
   ],
   "source": [
    "from src.data.datasets import get_cifar10\n",
    "dataset_params = {'dataset_path': None, 'whether_aug': True, 'proper_normalization': True, 'subset_path': 'data/cifar10_idxs.npy'}\n",
    "datasets = get_cifar10(**dataset_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "dataset_path =  os.environ['CIFAR10_PATH']\n",
    "train_dataset = datasets.CIFAR10(dataset_path, train=True, download=True)\n",
    "\n",
    "selected_indices = torch.tensor(np.load(subset_path))\n",
    "train_dataset = torch.utils.data.Subset(train_dataset, selected_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([25000])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.Subset at 0x7f2521c99a50>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet-18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/NFS_SHARE/home/b.krzepkowski/miniconda3/envs/ncollapse/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from src.utils.prepare import prepare_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {'backbone_type': 'resnet18',\n",
    "                    'only_features': False,\n",
    "                    'batchnorm_layers': True,\n",
    "                    'width_scale': 1.0,\n",
    "                    'skips': True,\n",
    "                    'modify_resnet': True}\n",
    "model_params = {'model_config': model_config, 'num_classes': 10, 'dataset_name': 'cifar10'}\n",
    "\n",
    "model = prepare_model('resnet_tunnel', model_params=model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): Identity()\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fp2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
