{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import InterpolationMode\n",
    "mean, std = (0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.262)\n",
    "transform_train_proper = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std),\n",
    "        ])\n",
    "transform_blurred = transforms.Compose([\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Resize(8, interpolation=InterpolationMode.BILINEAR, antialias=None), \n",
    "    transforms.Resize(32, interpolation=InterpolationMode.BILINEAR, antialias=None),\n",
    "    transforms.Normalize(mean, std)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.datasets import get_cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import utils\n",
    "DATASET_NAME = 'cifar10'\n",
    "\n",
    "def get_held_out_data(dataset_name, nb_samples=50):\n",
    "    train_dataset, _, _ = get_cifar10()\n",
    "    x_data = np.array(train_dataset.data)\n",
    "    y_data = np.array(train_dataset.targets)\n",
    "    num_classes = len(np.unique(y_data))\n",
    "    nb_samples_per_class = nb_samples // num_classes\n",
    "    idxs = []\n",
    "    for i in range(num_classes):\n",
    "        idxs_i = np.where(y_data == i)[0]\n",
    "        sampled_idxs_i = np.random.choice(idxs_i, size=nb_samples_per_class, replace=False)\n",
    "        idxs.append(sampled_idxs_i)\n",
    "        \n",
    "    idxs = np.concatenate(idxs)\n",
    "    x_data = x_data[idxs]\n",
    "    y_data = y_data[idxs]\n",
    "    \n",
    "    if not os.path.exists('data'):\n",
    "        os.mkdir('data')\n",
    "    np.save(f'data/{dataset_name}_held_out_x.npy', x_data)\n",
    "    np.save(f'data/{dataset_name}_held_out_y.npy', y_data)\n",
    "    return x_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "x_data, y_data = get_held_out_data(DATASET_NAME, nb_samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data_proper = [transform_train_proper(x) for x in x_data]\n",
    "x_data_proper = torch.stack(x_data_proper)\n",
    "y_data_proper = torch.from_numpy(y_data)\n",
    "\n",
    "torch.save(x_data_proper, f'data/{DATASET_NAME}_held_out_proper_x.pt')\n",
    "torch.save(y_data_proper, f'data/{DATASET_NAME}_held_out_proper_y.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data_blurred = [transform_blurred(x) for x in x_data]\n",
    "x_data_blurred = torch.stack(x_data_blurred)\n",
    "y_data_blurred = torch.from_numpy(y_data)\n",
    "\n",
    "torch.save(x_data_blurred, f'data/{DATASET_NAME}_held_out_blurred_x.pt')\n",
    "torch.save(y_data_blurred, f'data/{DATASET_NAME}_held_out_blurred_y.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.prepare import prepare_loaders\n",
    "batch_size = 500\n",
    "loaders_params = {'batch_size': batch_size, 'pin_memory': True, 'num_workers': 8}\n",
    "loaders = prepare_loaders(DATASET_NAME, {}, loaders_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(loaders['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(y.cpu().numpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = x_data_proper.reshape(x_data_proper.size(0), -1)\n",
    "x_data /= x_data.norm(dim=1, keepdim=True)\n",
    "sim_m = x_data @ x_data.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_info(cluster_labels, y_train):\n",
    "    # Initializing\n",
    "    unsolicited_ratio = 0.0\n",
    "    denominator = 0.0\n",
    "    reference_labels = {}\n",
    "    # For loop to run through each label of cluster label\n",
    "    for label in range(len(np.unique(y_train))):\n",
    "        index = np.where(cluster_labels==label, 1, 0)\n",
    "        dist = np.bincount(y_train[index==1])\n",
    "        num = dist.argmax()\n",
    "        unsolicited_ratio += (dist.sum() - dist.max())\n",
    "        denominator += dist.sum()\n",
    "        reference_labels[label] = num\n",
    "    proper_labels = [reference_labels[label] for label in cluster_labels]\n",
    "    proper_labels = np.array(proper_labels)\n",
    "    unsolicited_ratio /= denominator\n",
    "    return proper_labels, unsolicited_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering \n",
    "\n",
    "similarity_matrix_ = sim_m.cpu().numpy()\n",
    "labels_pred = SpectralClustering(n_clusters=10, affinity='precomputed', n_init=100, assign_labels='discretize').fit_predict((1+similarity_matrix_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_pred, unsolicited_ratio = retrieve_info(labels_pred, y_data)\n",
    "unsolicited_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data_proper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = (labels_pred == y_data_proper.numpy()).astype(float).sum() / y_data_proper.shape[0]\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(labels_pred == y_data_proper.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([4, 5, 6]),\n",
       "indices=tensor([1, 1, 1]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[1, 2, 3], [4, 5, 6]]).max(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root=os.environ['CIFAR10_PATH'], train=True,\n",
    "                                        download=False, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root=os.environ['CIFAR10_PATH'], train=False,\n",
    "                                       download=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(iter(testloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.025"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2.5e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0., 160., 320., 480., 640., 800.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.linspace(0, 800, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fp2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
